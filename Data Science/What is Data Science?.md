 # Module 1
data science is study of data to uncover new insights and knowledge from analysing data/ exploring data with help of high computing power.

data --> stories --> Insights

problem --> data collection --> analysis --> pattern recognition --> story telling --> visualisation

which help to 

1. Understand the environment where the data came from
2. Analyse the existing issues
3. Reveal previously hidden opportunities.


Data Analysis:

data analysis helps to increase knowledge (value) of the organisation about their environment.

working model:

1. Specific question -  clarify the question -  what things needed to be answered / problem to be solved?
2. what data do we need to solve the problem?
3. where it will come from?
4. structured and unstructured data analysis
5. reveal outliers, patterns and new knowledge 
6. story telling - explain data insights using powerful visual tools - understand the nature of the results - recommended action to take


### Paths to data science

	data science used to be profession of statistics and mathematics.
	then it evolved to became as data science due to increasing number of data. 
	and the evolution of modern cpu with high processing power.

 important things to know --> basic knowledge in some industry --> mathematics (linear algebra, calculus, statistics) --> programming python -->

### Roadmap for Machine Learning Engineer
	A roadmap for becoming a machine learning (ML) engineer typically involves several stages, from foundational knowledge to specialized skills and hands-on experience. Hereâ€™s a comprehensive guide:


---
1. Foundational Knowledge

**Mathematics and Statistics**
- Linear Algebra: Vectors, matrices, operations, eigenvalues, and eigenvectors.
- Calculus: Derivatives, integrals, partial derivatives, gradients.
- Probability and Statistics: Distributions, Bayes  theorem, hypothesis testing, statistical
significance.
**Programming**
- Python: Master the basics and libraries like NumPy, Pandas, Matplotlib, and Seaborn.
- R: Optional but useful for statistical analysis and data visualisation.
---
1. Core Machine Learning Concepts

**Fundamental Algorithms**
- Supervised Learning: Linear regression, logistic regression, decision trees, support vector
machines, k-nearest neighbours.
- Unsupervised Learning: K-means clustering, hierarchical clustering, principal component analysis
(PCA).
- Reinforcement Learning: Markov decision processes, Q-learning.Machine Learning Libraries
- Scikit-Learn: For implementing basic ML algorithms and techniques.
- TensorFlow/PyTorch: For deep learning applications.
---
3. Data Preprocessing and Visualisation

Data Cleaning: Handling missing values, outliers, and data normalisation.
Feature Engineering: Creating new features, encoding categorical variables.
Data Visualisation: Using tools like Matplotlib, Seaborn, and Plotly for exploratory data analysis.

---
4. Advanced Machine Learning and Deep Learning

Deep Learning
- Neural Networks: Understanding architectures, activation functions, back propagation.
- Convolutional Neural Networks (CNNs): For image data.
- Recurrent Neural Networks (RNNs): For sequential data.
- Generative Adversarial Networks (GANs): For generative tasks.
- Transfer Learning: Using pre-trained models for new tasks.
Advanced Topics
- Natural Language Processing (NLP): Text preprocessing, embeddings, transformers (e.g., BERT,
GPT).
- Time Series Analysis: ARIMA models, LSTMs for forecasting.
- Anomaly Detection: Techniques for identifying unusual patterns.

---
1. Model Deployment and ProductionizationModel Evaluation: Metrics like accuracy, precision, recall, F1-score, ROC-AUC.

Model Tuning: Hyperparameter optimization using grid search, random search, and Bayesian
optimization.
Deployment: Using Flask/Django for creating APIs, Docker for containerization, and cloud services
(AWS, GCP, Azure) for scalable deployment.
MLOps: CI/CD pipelines for ML models, monitoring, and maintaining models in production.
6. Practical Experience
Projects and Competitions
- Kaggle Competitions: Participate to apply your skills and learn from others.
- Personal Projects: Build projects to solve real-world problems and create a portfolio.
Internships and Jobs
- Internships: Gain practical experience by working with experienced ML engineers.
- Entry-level Positions: Start as a junior data scientist or ML engineer to further develop your skills.

---
1. Continuous Learning and Specialization

Advanced Courses and Certifications
- Online Courses: Platforms like Coursera, Udacity, and edX offer advanced ML and AI courses.
- Certifications: Obtain certifications from reputable institutions to validate your skills.
Stay Updated
- Research Papers: Read papers from conferences like NeurIPS, ICML, and CVPR.- Communities: Join ML communities on GitHub, Stack Overflow, and Reddit.
- Blogs and Podcasts: Follow experts in the field to stay current with the latest trends.
8. Soft Skills
- Problem-Solving: Critical for developing effective ML solutions.
- Communication: Ability to explain complex concepts to non-technical stakeholders.
- Collaboration: Work effectively with cross-functional teams.
Conclusion
Becoming a proficient ML engineer is a journey that requires a mix of theoretical knowledge,
practical experience, and continuous learning. By following this roadmap, you can systematically
build the skills and experience needed to excel in the field of machine learning.


### new data scientist

1. curious 
2. argumentative 
3. judgemental
4. analytics platform
5. Expertise  - what kinda field that you know / curious or passion about /  always engaged in

### to become skilled

1. versatility
2. subject area knowledge
3. programming experience
4. comfortable with maths
5. curious 
6. storyteller
7. diverse background
8. adept suitable tools
9. problem solving



<table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>

</tr>
</thead>
<tbody><tr>
<td>Algorithms</td>
<td>A set of step-by-step instructions to solve a problem or complete a task.</td>

</tr>
<tr>
<td>Model</td>
<td>A representation of the relationships and patterns found in data to make predictions or analyze complex systems retaining essential elements needed for analysis.</td>

</tr>
<tr>
<td>Outliers</td>
<td>When a data point or points occur significantly outside of most of the other data in a data set, potentially indicating anomalies, errors, or unique phenomena that could impact statistical analysis or modeling.</td>

</tr>
<tr>
<td>Quantitative analysis</td>
<td>A systematic approach using mathematical and statistical analysis is used to interpret numerical data.</td>

</tr>
<tr>
<td>Structured data</td>
<td>Data is organized and formatted into a predictable schema, usually related tables with rows and columns.</td>

</tr>
<tr>
<td>Unstructured data</td>
<td>Unorganized data that lacks a predefined data model or organization makes it harder to analyze using traditional methods. This data type often includes text, images, videos, and other content that doesn’t fit neatly into rows and columns like structured data.</td>

</tr>
</tbody></table>



### what data scientists do

mostly solving some issues with a help of analysis data.
### types of file formats

1. delimited text file formats - .CSV -->  data as text, each value separated by a delimiter (comma, tab, colon, vertical bar, space).
2. Microsoft Excel Open .XML spreadsheet, Or .XLSX -->xml based file format, mutliple worksheets, accessible by most applications, secure
3. Extensible markup  language - .XML --> markup language with set rules for encoding data, readable by human and machines, self-descriptive language, platform and programming independent
4. Portable Document format - .PDF --> documents for independent of application, hardware, operating system, viewed by any device
5. JavaScript object notation - JSON --> text based data transmission over the web, language independent data format, readable by any programming language, best tools for sharing wide type of data (text, audio and video)
   


### tools and algorithms




# Module 2

### Digital transformation

digital transformation is updating or improving the pre- existing services and operating with insights from data analysis. due to availability of data and improvements in computing power of the modern CPU's gave a new way to improve pre existing services using data science. 

#### examples:
1. Netflix recommended systems
2. NBA team -  Houston rockets point scoring analysis
3. Lufthansa flights improvements in their services

### Cloud

	it is the delivery of on-demand computing resources such as
	1. Networks
	2. servers
	3. storage
	4. services
	5. applications
	6. data centers
	over the interent on a pay-for-use basis

#### Cloud computing

	applications and data that users access over the internet rather than locally

		examples
		1. online web apps
		2. secure online business applications
		3. storing personal files
		Benefits:
		1. no need for offline applications
		2. use latest version of online apps with low monthly subscription
		3. less local storage
		4. work collaboratively
	mainly cloud computing consists of 
		 1. five essential characteristics
		 2. three deployment models
		 3. three service models
#### Five important characteristics

1. On-demand self-service - access to processing power, storage, network and simple user interface without interaction with service provider
2. Broad network access - can be accessed through different mediums from different countries through internet
3. Resource pooling -  multitenent model --> pooled resources used for multiple consumers which are dynamically assigned according to demand
4. Raid elasticity - consumer can increase their demand for resources which are elastically provisioned and released
5. measured service -  pay for what you use --> which monitored, measured and reported transparently based on consumer utilisation

#### Cloud deployment models - public, private and hybrid

public cloud - consumer access cloud services over the internet and public cloud usage is shared by other companies
private cloud - cloud infrastructure provisioned for exclusive use by a single organisation
hybrid cloud - public and private cloud working seamlessly together

#### Cloud service models - Infrastructure, platform and application
Iaas - you can access the infrastructure level access as a  cloud service which includes physical computing resources

Paas - platform level that comprises the hardware and software tools needed to develop and deploy applications

Saas - application level that comprises the application as cloud service ( on-demand software)


### Big data

five important elements of big data

1. velocity - speed of which data accumulates -->YouTube video upload rate
2. volume - scale of data --> 7 billion people creates data 2.5 quintillion bytes amount of data everyday
3. veracity - quality of data --> consistency, completeness, integrity, ambiguity -->80% data unstructured -->reliable and accurate
4. variety - diversity of data (different data types, sources where it collected) -->
5. value - ability or need to turn data into value

	Apache spark and Hadoop helps to analyse and process big data with the process of distributing the data to different computer resources


### Big data processing tools

	tools and technologies helps to process the large scales structured, semi-structured and unstrucutred data

1. Apache hadoop
2. apache hive
3. apache spark


 #### Apache hadoop
 
	java based distributed storage and processing of big data across clusters of computers. scalable reliable cost-effective solution for data with no format requirements.
	
	 collection of tools that provides distributed storage and processing of big data
##### benefits:
1. better real-time data driven decisions
2. improved data access and analysis
3. data offload and consolidation.

##### tools included in Hadoop

	1. hadoop distributed file system (hdfs)--> multiple commodity hardware connected through a network. scalable and reliable storage by partitioning files over multiple nodes with parallel access and replicates file blocks on different nodes to prevent data loss. fast recovery from hardware failures, access to streaming data (high throughput rates), hdfs scales from nodes to clusters, compatible with different os and hardware platforms

#### Apache Hive

	it is datawarehouse for data query and analysis. open source date warehouse software for reading writing and managing larger data set files that are stored in HDFS or apache HBase.

	hive is based on hadoop and read based  . queries have high latency. not suitable for high volume of write operations. good for data warehousing tasks like ETL (extract - transform and load), reporting and data analysis. 

	access data via SQL.

 #### Apache Spark

	it is distributed analytics framework for complex, real time data analytics

	data processing engine designed to extract and process large volume of data for interactive analytics, stream processing, machine learning, data integration and ETL

	1. in-memory processing which increases the speed of computations
	2. provide interfaces for major programming languages like jave python r sql scala
	3. can run using standalone clustering technology
	4. and also run top of other infrastrutures like hadoop
	5. can access data from variety of sources like HDFS and Hive




### Data Mining


1. goal -->cost-benefit trade-off essential --> accuracy and reliability of the data
2. selecting data --> quality of data and cost is crucial
3. preprocessing data --> missing data, integrity of data columns, selecting correct attributes for further processing
4. transforming data --> storing minimal data to explain important phenomena. PCA, may convert data type (continuous to categorical)
5. storing data-->store data in database which facilitates efficient read and write operations for data scientists
6. Insights -->data analysis methods including parametric and non-parametric methods, machine learning methods
7. evaluating the results--> further improvements



