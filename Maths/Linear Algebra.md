<h3> Vector </h3>
<p>Vector has length and direction. Linear algebra involves functions of many variables. It's an ordered array</p>
<h3>Length</h3>
<p>Eulidean Length - square root of sum of squared components of that vector.
every vector representation starts from origin Zero (0,0).  but vector can start from anywhere on the geometrically space. we are using origin to understand linear algebra better.</p>

 <h3>Unit base vector</h3>
<p>Unit base vector represents basic unit of vector in the each axis. if **i** represents unit base  vector for axis-X, means it has one length and lies on the x axis. its other axis-lengths are zero (1,0). We can represent every vectors as scallers of the unit base vectors of its axis.
for example: [3,2]--> 3i + 2j. i and j are the unit base vectors of the axis-X and axis-Y respectively.</p>
<h3>Vector Addition</h3>
<ol>
<li> Let's recall the geometric interpretation of vector addition.</li>

<li>You start with a vector x that you want to add to vector y.</li>
<li>If you lay these head to toe, then the net result is the vector x plus y.</li>
<li> If we duplicate y and x by moving them around, we can create a parallelogram.</li>
<li> And sometimes, this picture right here where this diagonal of this parallelogram becomes x plus y is known as the parallelogram method for
vector addition.</li></ol>

<h3> Vector Substraction</h3>

<p>
1. , the diagonal becomes x plus y.
2. You can do the same thing for vector subtraction.
3. You lay out your vectors.
4. And then, the other diagonal becomes the vector x minus y.
5. Obviously, you have to make sure that it points in the right direction.
6. Now, how do you compute x minus y?
7. Well, you expose the different components of vectors x and y.
8. And you simply subtract each component of y off the corresponding
9. component of x.
</p>

<h3>Scaling</h3>
jn




<h2> Vectors and Linear Transformations </h2>

<h3> Neural Network and system of linear equations </h3>
<p> Neural networks are layers of linear models connected together from the input layer to the output layer. Every layer has number of Linear models called artificial neuron.  This artificial neuron uses the previous layer's output / input (in case of the first layer) to create linear model ( W*X + b). Artificial neuron uses the activation function on this linear model to produce new vector of input (a) for next layer. Every artificial neuron creates new vector which used as a input for the next layer. Each of the artificial creates unique vector from the given input.</p>

<h3>Vector and its properties</h3>

Vector can be represented as arrow in a plane. it has magnitude (size) and direction.  

L1_norm (x,y) = |x| +  |y| 
L2_norm (x,y) = $$\sqrt{(x*x) + (y*y)}$$



tan (theta) = y / x 
theta = arctan(y/x) gives the angle the vector on that plane

<h3> Vector operations</h3>

